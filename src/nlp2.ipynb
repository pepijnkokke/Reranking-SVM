{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import with_statement\n",
    "\n",
    "import collections\n",
    "import gzip\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Spacy library\n",
    "\n",
    "[Spacy](http://spacy.io) is an NLP library which is ready to be used in production settings. This library will help us find features for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "en_nlp = spacy.load('en')\n",
    "de_nlp = spacy.load('de')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the NLTK library\n",
    "\n",
    "We will use the [NLTK library](NLTK) for other features we want to extract from our data. As well as use the BLUE scoring algorithm in the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_s = u'Hello, world. Here are two sentences.'\n",
    "de_s = u'Ich bin ein Berliner.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BLEU(reference,candidate):\n",
    "    \"\"\"\n",
    "    Compute the BLEU score for a given candidate sentence, with respect to a\n",
    "    given reference sentence.\n",
    "\n",
    "    reference: the reference translation\n",
    "    candidate: the candidate translation\n",
    "    \"\"\"\n",
    "    return float(\n",
    "        nltk.translate.bleu_score.modified_precision([reference],candidate,n=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pos_feature(s,nlp):\n",
    "    \"\"\"\n",
    "    Compute the POS feature vector given a sentence and an instance of spaCy.\n",
    "    The POS feature vector is a vector which indicates, per POS-tag of the\n",
    "    language, what ratio of the words in the sentence have this POS-tag.\n",
    "\n",
    "    s  : input sentence\n",
    "    nlp: instance of spaCy nlp\n",
    "    \"\"\"\n",
    "    doc       = nlp(s,tag=True,parse=False,entity=False)\n",
    "    pos_count = collections.Counter([tok.tag_ for tok in doc])\n",
    "    return map(lambda tag: pos_count[tag] / len(doc), nlp.tagger.tag_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "[0.0, 0.0, 0.0, 0.0, 0.11, 0.0, 0.0, 0.22, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11, 0.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Example: print the POS-feature for an English sentence.\n",
    "print(len(en_nlp.tagger.tag_names))\n",
    "print(map(lambda x: round(x,2),pos_feature(en_s,en_nlp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "[0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Example: print the POS-feature for a German sentence.\n",
    "print(len(de_nlp.tagger.tag_names))\n",
    "print(map(lambda x: round(x,2),pos_feature(de_s,de_nlp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "[ 0.07028833 -0.00575205  0.02307617  0.0293571   0.0150436   0.03114259\n",
      "  0.00853428  0.00191485  0.00394045 -0.02803375  0.04562261 -0.00354294\n",
      " -0.04505194  0.0158382  -0.01503225 -0.06313571 -0.06844621 -0.06096174\n",
      " -0.0268664  -0.0046172  -0.031034   -0.00546252  0.0032963  -0.04281867]\n"
     ]
    }
   ],
   "source": [
    "# Example: print the first 24 values in the sentence vector for an English sentence.\n",
    "en_doc = en_nlp(en_s)\n",
    "print(len(en_doc.vector))\n",
    "print(en_doc.vector[:24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "[ 0.04531119 -0.0290378   0.23427622  0.17914079 -0.19891641  0.1339044\n",
      " -0.16393779 -0.40263137 -0.04548381 -0.20328541  0.22776499 -0.1040916\n",
      " -0.1286144   0.078522    0.16846181 -0.33073679  0.34232721 -0.29092079\n",
      " -0.17509019  0.0541686   0.0833232  -0.27929959  0.0028818   0.15957041]\n"
     ]
    }
   ],
   "source": [
    "# Example: print the first 24 values in the sentence vector for a German sentence.\n",
    "de_doc = de_nlp(de_s)\n",
    "print(len(de_doc.vector))\n",
    "print(de_doc.vector[:24])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create constants for the paths to all data files.\n",
    "DATA_DIR         = os.path.abspath(os.path.join('..','data'))\n",
    "BASELINE_WEIGHTS = os.path.join(DATA_DIR,'baseline.weights')\n",
    "DEV_BEST         = os.path.join(DATA_DIR,'nlp2-dev.1000best')\n",
    "DEV_DE           = os.path.join(DATA_DIR,'nlp2-dev.de')\n",
    "DEV_EN_PLF       = os.path.join(DATA_DIR,'nlp2-dev.en.pw.plf-100')\n",
    "DEV_EN           = os.path.join(DATA_DIR,'nlp2-dev.en.s')\n",
    "TEST_BEST        = os.path.join(DATA_DIR,'nlp2-test.1000best')\n",
    "TEST_DE          = os.path.join(DATA_DIR,'nlp2-test.de')\n",
    "TEST_EN_PLF      = os.path.join(DATA_DIR,'nlp2-test.en.pw.plf-100')\n",
    "TEST_EN          = os.path.join(DATA_DIR,'nlp2-test.en.s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_candidate(s):\n",
    "    \"\"\"\n",
    "    Parse a candidate translation (a line from the 1000-best files) into\n",
    "    a tuple containing (in order):\n",
    "    \n",
    "        k:              the 0-based sentence id           (int)\n",
    "        source:         the source sentence               (str)\n",
    "        target:         the translated sentence           (str)\n",
    "        segments:       the segments and their alignments (list[(str,(int,int))])\n",
    "        feature_vector: the feature vector                ({str: list[float]})\n",
    "        score:          the score assigned by MOSES       (float)\n",
    "        alignments:     the alignments                    ([(int,int)])\n",
    "\n",
    "    Note: alignments in the \"segments\" field are pairs of states in the\n",
    "    input lattice, whereas the alignments in the \"alignments\" field are\n",
    "    pairs of a state in the input lattice together with the position of\n",
    "    the output word.\n",
    "    \"\"\"\n",
    "    k, segments_and_alignments, feature_vector, score, alignments, source = s.split(' ||| ')\n",
    "    \n",
    "    # Parse an id as an integer\n",
    "    k = int(k)\n",
    "    \n",
    "    # Parse a candidate translation (with alignments) into a sentence.\n",
    "    segments_and_alignments = map(lambda s: s.strip(),\n",
    "                                  re.split(r'\\|(\\d+\\-\\d+)\\|', segments_and_alignments))\n",
    "    segments = segments_and_alignments[0::2]\n",
    "    target = ' '.join(segments)\n",
    "    \n",
    "    # Parse a candidate translation (with alignments) into a list of segments.\n",
    "    segment_alignments = map(lambda s: tuple(map(int,s.strip().split('-'))), \n",
    "                             segments_and_alignments[1::2])\n",
    "    segments = zip(segments,segment_alignments)\n",
    "    \n",
    "    # Parse a feature vector string into a dictionary.\n",
    "    feature_vector = re.split(r'([A-Za-z]+0?)=', feature_vector)\n",
    "    feature_names  = feature_vector[1::2]\n",
    "    feature_values = map(lambda s: map(float,s.strip().split()), feature_vector[2::2])\n",
    "    feature_map = dict(zip(feature_names,feature_values))\n",
    "    \n",
    "    # Parse a score as a float.\n",
    "    score = float(score)\n",
    "    \n",
    "    # Parse an alignment string into a list of tuples.\n",
    "    alignments = map(lambda s: tuple(map(int,s.split('-'))), alignments.strip().split(' '))\n",
    "    \n",
    "    return (k, source, target, segments, feature_map, score, alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the development data.\n",
    "dev_limit = 1000\n",
    "\n",
    "with open(DEV_EN, 'r') as f:\n",
    "    inputs = [f.readline() for i in range(0, dev_limit)]\n",
    "    \n",
    "with open(DEV_DE, 'r') as f:\n",
    "    references = [f.readline() for i in range(0, dev_limit)]\n",
    "    \n",
    "with open(DEV_BEST,'r') as f:\n",
    "    candidates = []\n",
    "    candidate_set = []\n",
    "    i = 0\n",
    "    while True:\n",
    "        candidate = parse_candidate(f.readline())\n",
    "        if candidate[0] == i:\n",
    "            candidate_set.append(candidate)\n",
    "        else:\n",
    "            candidates.append(candidate_set)\n",
    "            candidate_set = [candidate]\n",
    "            i = candidate[0]\n",
    "        if i > dev_limit:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the development data.\n",
    "test_limit = 100\n",
    "\n",
    "with open(TEST_EN, 'r') as f:\n",
    "    test_inputs = [f.readline() for i in range(0, dev_limit)]\n",
    "    \n",
    "with open(TEST_DE, 'r') as f:\n",
    "    test_references = [f.readline() for i in range(0, dev_limit)]\n",
    "    \n",
    "with open(TEST_BEST, 'r') as f:\n",
    "    test_candidates = []\n",
    "    candidate_set = []\n",
    "    i = 0\n",
    "    while True:\n",
    "        candidate = parse_candidate(f.readline())\n",
    "        if candidate[0] == i:\n",
    "            candidate_set.append(candidate)\n",
    "        else:\n",
    "            test_candidates.append(candidate_set)\n",
    "            candidate_set = [candidate]\n",
    "            i = candidate[0]\n",
    "        if i > dev_limit:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two sets of lights so close to one another : intentional or just a silly error ?\n",
      "\n",
      "Zwei Anlagen so nah beieinander : Absicht oder Schildbürgerstreich ?\n",
      "\n",
      "[[0.0], [-88.3261], [10.0], [-27.4501, -35.3568, -17.6299, -29.1119], [0.838325, 0.0794625, 0.0794625], [0.0, 0.0, 0.0, 0.0], [-14.0], [-107.313], [0.0]]\n"
     ]
    }
   ],
   "source": [
    "# Example: print all relevant information for sentence with id #2.\n",
    "print(inputs[2])\n",
    "print(references[2])\n",
    "(k, source, target, segments, feature_map, score, alignments) = candidates[2][0]\n",
    "# print(segments)\n",
    "print(feature_map.values())\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing feature vector for two sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def feature_vector(e, c1, c2):\n",
    "    (_, _, t1, _, f1, s1, _) = c1\n",
    "    (_, _, t2, _, f2, s2, _) = c2\n",
    "    \n",
    "    return sum(f1.values(), []) + sum(f2.values(), []) + [s1, s2]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, -88.3261, 10.0, -27.4501, -35.3568, -17.6299, -29.1119, 0.838325, 0.0794625, 0.0794625, 0.0, 0.0, 0.0, 0.0, -14.0, -107.313, 0.0, 0.0, -89.0222, 10.0, -27.9478, -37.5129, -15.8729, -29.8851, 0.844537, 0.07325, 0.07325, 0.0, 0.0, 0.0, 0.0, -14.0, -107.313, 0.0]\n"
     ]
    }
   ],
   "source": [
    "feature_vector = feature_vector(inputs[2],candidates[2][0],candidates[2][1])\n",
    "print(feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_label(ref, c1, c2):\n",
    "    \n",
    "    (_, _, t1, _, _, _, _) = c1\n",
    "    (_, _, t2, _, _, _, _) = c2\n",
    "    \n",
    "    if (BLEU(ref, t1) > BLEU(ref, t2)):\n",
    "        return 1\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "training_label = training_label(references[2], candidates[2][0], candidates[2][500])\n",
    "print(training_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The PRO algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pro_corpus(inputs, references, candidates, sample_size=10):\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for i, e in enumerate(inputs):\n",
    "        g = references[i]\n",
    "        c = candidates[i]\n",
    "        data = data + pro(e, g, c, sample_size)\n",
    "    \n",
    "    (x,y) = zip(*data)\n",
    "    return (list(x), list(y))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pro(e, g, c, sample_size=10):\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for i in range(0,sample_size):\n",
    "        \n",
    "        # Randomly pick two candidates that are not the same\n",
    "        j1 = j2 = random.randint(0,len(c)-1)\n",
    "        while j1 == j2:\n",
    "            j2 = random.randint(0,len(c)-1)\n",
    "            \n",
    "        training_example = ( feature_vector(e,c[j1],c[j2]), training_label(g,c[j1],c[j2]) )\n",
    "        \n",
    "        data.append(training_example)\n",
    "    \n",
    "    return data\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(train_x, train_y) = pro_corpus(inputs, references, candidates, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.LinearSVC()\n",
    "clf.fit(train_x, train_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.98      0.67      2541\n",
      "          1       0.61      0.03      0.05      2459\n",
      "\n",
      "avg / total       0.56      0.51      0.37      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generating test data    \n",
    "(test_x, test_y) = pro_corpus(test_inputs, test_references, test_candidates, 5)\n",
    "predicted = clf.predict(test_x)\n",
    "print(metrics.classification_report(test_y, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Reranking the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def best_sentences(inputs, candidates, clf):\n",
    "    sentences = []\n",
    "    \n",
    "    for i, e in enumerate(inputs):\n",
    "        j = i + 1000\n",
    "        c = candidates[i]\n",
    "        sentences.append(best_sentence(e, c, clf))\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def best_sentence(e, c, clf):\n",
    "    \n",
    "    def compare(x, y):\n",
    "        if clf.predict([feature_vector(e,x,y)]) == [0]:\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "    (_, _, s, _, _, _, _) = sorted(c, cmp=compare)[0]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bleu_references = [[x] for x in test_references]\n",
    "bleu_hypotheses = best_sentences(test_inputs, test_candidates, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.594249827189\n"
     ]
    }
   ],
   "source": [
    "blue = nltk.translate.bleu_score.corpus_bleu(bleu_references, bleu_hypotheses) \n",
    "print(blue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
