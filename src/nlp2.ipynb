{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import with_statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Spacy library\n",
    "\n",
    "[Spacy](http://spacy.io) is an NLP library which is ready to be used in production settings. This library will help us find features for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "en_nlp = spacy.load('en')\n",
    "de_nlp = spacy.load('de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named nltk",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-53fba42cc3a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named nltk"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import gzip\n",
    "import nltk\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_s = u'Hello, world. Here are two sentences.'\n",
    "de_s = u'Ich bin ein Berliner.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BLEU(reference,candidate,n):\n",
    "    \"\"\"\n",
    "    Compute the BLEU score for a given candidate sentence, with respect to a\n",
    "    given reference sentence.\n",
    "\n",
    "    reference: the reference translation\n",
    "    candidate: the candidate translation\n",
    "    n        : the size of the ngrams\n",
    "    \"\"\"\n",
    "    return float(\n",
    "        nltk.translate.bleu_score.modified_precision([reference],candidate,n=n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pos_feature(s,nlp):\n",
    "    \"\"\"\n",
    "    Compute the POS feature vector given a sentence and an instance of spaCy.\n",
    "    The POS feature vector is a vector which indicates, per POS-tag of the\n",
    "    language, what ratio of the words in the sentence have this POS-tag.\n",
    "\n",
    "    s  : input sentence\n",
    "    nlp: instance of spaCy nlp\n",
    "    \"\"\"\n",
    "    doc       = nlp(s,tag=True,parse=False,entity=False)\n",
    "    pos_count = collections.Counter([tok.tag_ for tok in doc])\n",
    "    return map(lambda tag: pos_count[tag] / len(doc), nlp.tagger.tag_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example: print the POS-feature for an English sentence.\n",
    "print(len(en_nlp.tagger.tag_names))\n",
    "print(map(lambda x: round(x,2),pos_feature(en_s,en_nlp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example: print the POS-feature for a German sentence.\n",
    "print(len(de_nlp.tagger.tag_names))\n",
    "print(map(lambda x: round(x,2),pos_feature(de_s,de_nlp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example: print the first 24 values in the sentence vector for an English sentence.\n",
    "en_doc = en_nlp(en_s)\n",
    "print(len(en_doc.vector))\n",
    "print(en_doc.vector[:24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example: print the first 24 values in the sentence vector for a German sentence.\n",
    "de_doc = de_nlp(de_s)\n",
    "print(len(de_doc.vector))\n",
    "print(de_doc.vector[:24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create constants for the paths to all data files.\n",
    "DATA_DIR         = os.path.abspath(os.path.join('..','data'))\n",
    "BASELINE_WEIGHTS = os.path.join(DATA_DIR,'baseline.weights')\n",
    "DEV_BEST         = os.path.join(DATA_DIR,'nlp2-dev.1000best.gz')\n",
    "DEV_DE           = os.path.join(DATA_DIR,'nlp2-dev.de.gz')\n",
    "DEV_EN_PLF       = os.path.join(DATA_DIR,'nlp2-dev.en.pw.plf-100.gz')\n",
    "DEV_EN           = os.path.join(DATA_DIR,'nlp2-dev.en.s.gz')\n",
    "TEST_BEST        = os.path.join(DATA_DIR,'nlp2-test.1000best.gz')\n",
    "TEST_DE          = os.path.join(DATA_DIR,'nlp2-test.de.gz')\n",
    "TEST_EN_PLF      = os.path.join(DATA_DIR,'nlp2-test.en.pw.plf-100.gz')\n",
    "TEST_EN          = os.path.join(DATA_DIR,'nlp2-test.en.s.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the development data.\n",
    "with gzip.open(DEV_EN,  'r') as f: inputs     = f.readlines()\n",
    "with gzip.open(DEV_DE,  'r') as f: references = f.readlines()\n",
    "with gzip.open(DEV_BEST,'r') as f: candidates = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_candidate(s):\n",
    "    \"\"\"\n",
    "    Parse a candidate translation (a line from the 1000-best files) into\n",
    "    a tuple containing (in order):\n",
    "    \n",
    "        k:              the 0-based sentence id           (int)\n",
    "        source:         the source sentence               (str)\n",
    "        target:         the translated sentence           (str)\n",
    "        segments:       the segments and their alignments (list[(str,(int,int))])\n",
    "        feature_vector: the feature vector                ({str: list[float]})\n",
    "        score:          the score assigned by MOSES       (float)\n",
    "        alignments:     the alignments                    ([(int,int)])\n",
    "\n",
    "    Note: alignments in the \"segments\" field are pairs of states in the\n",
    "    input lattice, whereas the alignments in the \"alignments\" field are\n",
    "    pairs of a state in the input lattice together with the position of\n",
    "    the output word.\n",
    "    \"\"\"\n",
    "    k, segments_and_alignments, feature_vector, score, alignments, source = s.split(' ||| ')\n",
    "    \n",
    "    # Parse an id as an integer\n",
    "    k = int(k)\n",
    "    \n",
    "    # Parse a candidate translation (with alignments) into a sentence.\n",
    "    segments_and_alignments = map(lambda s: s.strip(),\n",
    "                                  re.split(r'\\|(\\d+\\-\\d+)\\|', segments_and_alignments))\n",
    "    segments = segments_and_alignments[0::2]\n",
    "    target = ' '.join(segments)\n",
    "    \n",
    "    # Parse a candidate translation (with alignments) into a list of segments.\n",
    "    segment_alignments = map(lambda s: tuple(map(int,s.strip().split('-'))), \n",
    "                             segments_and_alignments[1::2])\n",
    "    segments = zip(segments,segment_alignments)\n",
    "    \n",
    "    # Parse a feature vector string into a dictionary.\n",
    "    feature_vector = re.split(r'([A-Za-z]+0?)=', feature_vector)\n",
    "    feature_names  = feature_vector[1::2]\n",
    "    feature_values = map(lambda s: map(float,s.strip().split()), feature_vector[2::2])\n",
    "    feature_vector = dict(zip(feature_names,feature_values))\n",
    "    \n",
    "    # Parse a score as a float.\n",
    "    score = float(score)\n",
    "    \n",
    "    # Parse an alignment string into a list of tuples.\n",
    "    alignments = map(lambda s: tuple(map(int,s.split('-'))), alignments.strip().split(' '))\n",
    "    \n",
    "    return (k, source, target, segments, feature_vector, score, alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example: print all relevant information for sentence with id #2.\n",
    "print(inputs[2])\n",
    "print(references[2])\n",
    "for fld in parse_candidate(candidates[2000]):\n",
    "    print(fld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
