{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import with_statement\n",
    "\n",
    "import collections\n",
    "import gzip\n",
    "import os\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Spacy library\n",
    "\n",
    "[Spacy](http://spacy.io) is an NLP library which is ready to be used in production settings. This library will help us find features for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "en_nlp = spacy.load('en')\n",
    "de_nlp = spacy.load('de')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the NLTK library\n",
    "\n",
    "We will use the [NLTK library](NLTK) for other features we want to extract from our data. As well as use the BLUE scoring algorithm in the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_s = u'Hello, world. Here are two sentences.'\n",
    "de_s = u'Ich bin ein Berliner.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BLEU(reference,candidate,n):\n",
    "    \"\"\"\n",
    "    Compute the BLEU score for a given candidate sentence, with respect to a\n",
    "    given reference sentence.\n",
    "\n",
    "    reference: the reference translation\n",
    "    candidate: the candidate translation\n",
    "    n        : the size of the ngrams\n",
    "    \"\"\"\n",
    "    return float(\n",
    "        nltk.translate.bleu_score.modified_precision([reference],candidate,n=n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pos_feature(s,nlp):\n",
    "    \"\"\"\n",
    "    Compute the POS feature vector given a sentence and an instance of spaCy.\n",
    "    The POS feature vector is a vector which indicates, per POS-tag of the\n",
    "    language, what ratio of the words in the sentence have this POS-tag.\n",
    "\n",
    "    s  : input sentence\n",
    "    nlp: instance of spaCy nlp\n",
    "    \"\"\"\n",
    "    doc       = nlp(s,tag=True,parse=False,entity=False)\n",
    "    pos_count = collections.Counter([tok.tag_ for tok in doc])\n",
    "    return map(lambda tag: pos_count[tag] / len(doc), nlp.tagger.tag_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "[0.0, 0.0, 0.0, 0.0, 0.11, 0.0, 0.0, 0.22, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11, 0.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Example: print the POS-feature for an English sentence.\n",
    "print(len(en_nlp.tagger.tag_names))\n",
    "print(map(lambda x: round(x,2),pos_feature(en_s,en_nlp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "[0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Example: print the POS-feature for a German sentence.\n",
    "print(len(de_nlp.tagger.tag_names))\n",
    "print(map(lambda x: round(x,2),pos_feature(de_s,de_nlp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "[ 0.07028833 -0.00575205  0.02307617  0.0293571   0.0150436   0.03114259\n",
      "  0.00853428  0.00191485  0.00394045 -0.02803375  0.04562261 -0.00354294\n",
      " -0.04505194  0.0158382  -0.01503225 -0.06313571 -0.06844621 -0.06096174\n",
      " -0.0268664  -0.0046172  -0.031034   -0.00546252  0.0032963  -0.04281867]\n"
     ]
    }
   ],
   "source": [
    "# Example: print the first 24 values in the sentence vector for an English sentence.\n",
    "en_doc = en_nlp(en_s)\n",
    "print(len(en_doc.vector))\n",
    "print(en_doc.vector[:24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "[ 0.04531119 -0.0290378   0.23427622  0.17914079 -0.19891641  0.1339044\n",
      " -0.16393779 -0.40263137 -0.04548381 -0.20328541  0.22776499 -0.1040916\n",
      " -0.1286144   0.078522    0.16846181 -0.33073679  0.34232721 -0.29092079\n",
      " -0.17509019  0.0541686   0.0833232  -0.27929959  0.0028818   0.15957041]\n"
     ]
    }
   ],
   "source": [
    "# Example: print the first 24 values in the sentence vector for a German sentence.\n",
    "de_doc = de_nlp(de_s)\n",
    "print(len(de_doc.vector))\n",
    "print(de_doc.vector[:24])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create constants for the paths to all data files.\n",
    "DATA_DIR         = os.path.abspath(os.path.join('..','data'))\n",
    "BASELINE_WEIGHTS = os.path.join(DATA_DIR,'baseline.weights')\n",
    "DEV_BEST         = os.path.join(DATA_DIR,'nlp2-dev.1000best.gz')\n",
    "DEV_DE           = os.path.join(DATA_DIR,'nlp2-dev.de.gz')\n",
    "DEV_EN_PLF       = os.path.join(DATA_DIR,'nlp2-dev.en.pw.plf-100.gz')\n",
    "DEV_EN           = os.path.join(DATA_DIR,'nlp2-dev.en.s.gz')\n",
    "TEST_BEST        = os.path.join(DATA_DIR,'nlp2-test.1000best.gz')\n",
    "TEST_DE          = os.path.join(DATA_DIR,'nlp2-test.de.gz')\n",
    "TEST_EN_PLF      = os.path.join(DATA_DIR,'nlp2-test.en.pw.plf-100.gz')\n",
    "TEST_EN          = os.path.join(DATA_DIR,'nlp2-test.en.s.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the development data.\n",
    "with gzip.open(DEV_EN,  'r') as f: inputs     = f.readlines()\n",
    "with gzip.open(DEV_DE,  'r') as f: references = f.readlines()\n",
    "with gzip.open(DEV_BEST,'r') as f: raw_candidates = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_candidate(s):\n",
    "    \"\"\"\n",
    "    Parse a candidate translation (a line from the 1000-best files) into\n",
    "    a tuple containing (in order):\n",
    "    \n",
    "        k:              the 0-based sentence id           (int)\n",
    "        source:         the source sentence               (str)\n",
    "        target:         the translated sentence           (str)\n",
    "        segments:       the segments and their alignments (list[(str,(int,int))])\n",
    "        feature_vector: the feature vector                ({str: list[float]})\n",
    "        score:          the score assigned by MOSES       (float)\n",
    "        alignments:     the alignments                    ([(int,int)])\n",
    "\n",
    "    Note: alignments in the \"segments\" field are pairs of states in the\n",
    "    input lattice, whereas the alignments in the \"alignments\" field are\n",
    "    pairs of a state in the input lattice together with the position of\n",
    "    the output word.\n",
    "    \"\"\"\n",
    "    k, segments_and_alignments, feature_vector, score, alignments, source = s.split(' ||| ')\n",
    "    \n",
    "    # Parse an id as an integer\n",
    "    k = int(k)\n",
    "    \n",
    "    # Parse a candidate translation (with alignments) into a sentence.\n",
    "    segments_and_alignments = map(lambda s: s.strip(),\n",
    "                                  re.split(r'\\|(\\d+\\-\\d+)\\|', segments_and_alignments))\n",
    "    segments = segments_and_alignments[0::2]\n",
    "    target = ' '.join(segments)\n",
    "    \n",
    "    # Parse a candidate translation (with alignments) into a list of segments.\n",
    "    segment_alignments = map(lambda s: tuple(map(int,s.strip().split('-'))), \n",
    "                             segments_and_alignments[1::2])\n",
    "    segments = zip(segments,segment_alignments)\n",
    "    \n",
    "    # Parse a feature vector string into a dictionary.\n",
    "    feature_vector = re.split(r'([A-Za-z]+0?)=', feature_vector)\n",
    "    feature_names  = feature_vector[1::2]\n",
    "    feature_values = map(lambda s: map(float,s.strip().split()), feature_vector[2::2])\n",
    "    feature_map = dict(zip(feature_names,feature_values))\n",
    "    \n",
    "    # Parse a score as a float.\n",
    "    score = float(score)\n",
    "    \n",
    "    # Parse an alignment string into a list of tuples.\n",
    "    alignments = map(lambda s: tuple(map(int,s.split('-'))), alignments.strip().split(' '))\n",
    "    \n",
    "    return (k, source, target, segments, feature_map, score, alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two sets of lights so close to one another : intentional or just a silly error ?\n",
      "\n",
      "Zwei Anlagen so nah beieinander : Absicht oder Schildbürgerstreich ?\n",
      "\n",
      "[[-200.0], [-197.749], [24.0], [-46.6874, -49.1628, -45.9683, -57.3], [0.906734, 0.017208, 0.017208], [0.0, 0.0, 0.0, 0.0], [-32.0], [-247.261], [0.0]]\n"
     ]
    }
   ],
   "source": [
    "# Example: print all relevant information for sentence with id #2.\n",
    "print(inputs[2])\n",
    "print(references[2])\n",
    "(k, source, target, segments, feature_map, score, alignments) = parse_candidate(raw_candidates[1999])\n",
    "# print(segments)\n",
    "print(feature_map.values())\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing feature vector for two sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def feature_vector(e, c1, c2):\n",
    "    (_, _, t1, _, f1, _, _) = c1\n",
    "    (_, _, t2, _, f2, _, _) = c2\n",
    "    \n",
    "    return sum(f1.values(), []) + sum(f2.values(), [])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_label(ref, c1, c2):\n",
    "    \n",
    "    (_, _, t1, _, _, _, _) = c1\n",
    "    (_, _, t2, _, _, _, _) = c2\n",
    "    \n",
    "    if (BLEU(ref, t1, 4) > BLEU(ref, t2, 4)):\n",
    "        return 1\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The PRO algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pro_corpus(inputs, references, candidates, n=0, sample_size=10):\n",
    "    \n",
    "    training_data = []\n",
    "    \n",
    "    for i, e in enumerate(inputs):\n",
    "        if n == 0 or i < n: \n",
    "            g = references[i]\n",
    "            c = [parse_candidate(x) for x in candidates[i:(i + 1000)]]\n",
    "            training_data = training_data + pro(e, g, c, sample_size)\n",
    "        \n",
    "    return training_data\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pro(e, g, c, sample_size=10):\n",
    "    \n",
    "    training_data = []\n",
    "    \n",
    "    for i in range(0,sample_size):\n",
    "        \n",
    "        # Randomly pick two candidates that are not the same\n",
    "        j1 = j2 = random.randint(0,999)\n",
    "        while j1 == j2:\n",
    "            j2 = random.randint(0,999)\n",
    "            \n",
    "        training_example = ( feature_vector(e,c[j1],c[j2]), training_label(g,c[j1],c[j2]) )\n",
    "        \n",
    "        training_data.append(training_example)\n",
    "    \n",
    "    return training_data\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'candidates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-10bb7e4d421d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpro_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'candidates' is not defined"
     ]
    }
   ],
   "source": [
    "training_data = pro_corpus(inputs, references, raw_candidates, 50, 10)\n",
    "(x,y) = zip(*training_data)\n",
    "x = list(x)\n",
    "y = list(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVR()\n",
    "clf.fit(x, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
