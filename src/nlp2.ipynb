{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import with_statement\n",
    "\n",
    "import collections\n",
    "import gzip\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Spacy library\n",
    "\n",
    "[Spacy](http://spacy.io) is an NLP library which is ready to be used in production settings. This library will help us find features for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "en_nlp = spacy.load('en')\n",
    "de_nlp = spacy.load('de')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the NLTK library\n",
    "\n",
    "We will use the [NLTK library](NLTK) for other features we want to extract from our data. As well as use the BLUE scoring algorithm in the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_s = u'Hello, world. Here are two sentences.'\n",
    "de_s = u'Ich bin ein Berliner.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BLEU(reference,candidate,n):\n",
    "    \"\"\"\n",
    "    Compute the BLEU score for a given candidate sentence, with respect to a\n",
    "    given reference sentence.\n",
    "\n",
    "    reference: the reference translation\n",
    "    candidate: the candidate translation\n",
    "    n        : the size of the ngrams\n",
    "    \"\"\"\n",
    "    return float(\n",
    "        nltk.translate.bleu_score.modified_precision([reference],candidate,n=n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pos_feature(s,nlp):\n",
    "    \"\"\"\n",
    "    Compute the POS feature vector given a sentence and an instance of spaCy.\n",
    "    The POS feature vector is a vector which indicates, per POS-tag of the\n",
    "    language, what ratio of the words in the sentence have this POS-tag.\n",
    "\n",
    "    s  : input sentence\n",
    "    nlp: instance of spaCy nlp\n",
    "    \"\"\"\n",
    "    doc       = nlp(s,tag=True,parse=False,entity=False)\n",
    "    pos_count = collections.Counter([tok.tag_ for tok in doc])\n",
    "    return map(lambda tag: pos_count[tag] / len(doc), nlp.tagger.tag_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "[0.0, 0.0, 0.0, 0.0, 0.11, 0.0, 0.0, 0.22, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11, 0.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Example: print the POS-feature for an English sentence.\n",
    "print(len(en_nlp.tagger.tag_names))\n",
    "print(map(lambda x: round(x,2),pos_feature(en_s,en_nlp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "[0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Example: print the POS-feature for a German sentence.\n",
    "print(len(de_nlp.tagger.tag_names))\n",
    "print(map(lambda x: round(x,2),pos_feature(de_s,de_nlp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "[ 0.07028833 -0.00575205  0.02307617  0.0293571   0.0150436   0.03114259\n",
      "  0.00853428  0.00191485  0.00394045 -0.02803375  0.04562261 -0.00354294\n",
      " -0.04505194  0.0158382  -0.01503225 -0.06313571 -0.06844621 -0.06096174\n",
      " -0.0268664  -0.0046172  -0.031034   -0.00546252  0.0032963  -0.04281867]\n"
     ]
    }
   ],
   "source": [
    "# Example: print the first 24 values in the sentence vector for an English sentence.\n",
    "en_doc = en_nlp(en_s)\n",
    "print(len(en_doc.vector))\n",
    "print(en_doc.vector[:24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "[ 0.04531119 -0.0290378   0.23427622  0.17914079 -0.19891641  0.1339044\n",
      " -0.16393779 -0.40263137 -0.04548381 -0.20328541  0.22776499 -0.1040916\n",
      " -0.1286144   0.078522    0.16846181 -0.33073679  0.34232721 -0.29092079\n",
      " -0.17509019  0.0541686   0.0833232  -0.27929959  0.0028818   0.15957041]\n"
     ]
    }
   ],
   "source": [
    "# Example: print the first 24 values in the sentence vector for a German sentence.\n",
    "de_doc = de_nlp(de_s)\n",
    "print(len(de_doc.vector))\n",
    "print(de_doc.vector[:24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create constants for the paths to all data files.\n",
    "DATA_DIR         = os.path.abspath(os.path.join('..','data'))\n",
    "BASELINE_WEIGHTS = os.path.join(DATA_DIR,'baseline.weights')\n",
    "DEV_BEST         = os.path.join(DATA_DIR,'nlp2-dev.1000best.gz')\n",
    "DEV_DE           = os.path.join(DATA_DIR,'nlp2-dev.de.gz')\n",
    "DEV_EN_PLF       = os.path.join(DATA_DIR,'nlp2-dev.en.pw.plf-100.gz')\n",
    "DEV_EN           = os.path.join(DATA_DIR,'nlp2-dev.en.s.gz')\n",
    "TEST_BEST        = os.path.join(DATA_DIR,'nlp2-test.1000best.gz')\n",
    "TEST_DE          = os.path.join(DATA_DIR,'nlp2-test.de.gz')\n",
    "TEST_EN_PLF      = os.path.join(DATA_DIR,'nlp2-test.en.pw.plf-100.gz')\n",
    "TEST_EN          = os.path.join(DATA_DIR,'nlp2-test.en.s.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the development data.\n",
    "with gzip.open(DEV_EN,  'r') as f: inputs     = f.readlines()\n",
    "with gzip.open(DEV_DE,  'r') as f: references = f.readlines()\n",
    "with gzip.open(DEV_BEST,'r') as f: candidates = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_candidate(s):\n",
    "    \"\"\"\n",
    "    Parse a candidate translation (a line from the 1000-best files) into\n",
    "    a tuple containing (in order):\n",
    "    \n",
    "        k:              the 0-based sentence id           (int)\n",
    "        source:         the source sentence               (str)\n",
    "        target:         the translated sentence           (str)\n",
    "        segments:       the segments and their alignments (list[(str,(int,int))])\n",
    "        feature_vector: the feature vector                ({str: list[float]})\n",
    "        score:          the score assigned by MOSES       (float)\n",
    "        alignments:     the alignments                    ([(int,int)])\n",
    "\n",
    "    Note: alignments in the \"segments\" field are pairs of states in the\n",
    "    input lattice, whereas the alignments in the \"alignments\" field are\n",
    "    pairs of a state in the input lattice together with the position of\n",
    "    the output word.\n",
    "    \"\"\"\n",
    "    k, segments_and_alignments, feature_vector, score, alignments, source = s.split(' ||| ')\n",
    "    \n",
    "    # Parse an id as an integer\n",
    "    k = int(k)\n",
    "    \n",
    "    # Parse a candidate translation (with alignments) into a sentence.\n",
    "    segments_and_alignments = map(lambda s: s.strip(),\n",
    "                                  re.split(r'\\|(\\d+\\-\\d+)\\|', segments_and_alignments))\n",
    "    segments = segments_and_alignments[0::2]\n",
    "    target = ' '.join(segments)\n",
    "    \n",
    "    # Parse a candidate translation (with alignments) into a list of segments.\n",
    "    segment_alignments = map(lambda s: tuple(map(int,s.strip().split('-'))), \n",
    "                             segments_and_alignments[1::2])\n",
    "    segments = zip(segments,segment_alignments)\n",
    "    \n",
    "    # Parse a feature vector string into a dictionary.\n",
    "    feature_vector = re.split(r'([A-Za-z]+0?)=', feature_vector)\n",
    "    feature_names  = feature_vector[1::2]\n",
    "    feature_values = map(lambda s: map(float,s.strip().split()), feature_vector[2::2])\n",
    "    feature_map = dict(zip(feature_names,feature_values))\n",
    "    \n",
    "    # Parse a score as a float.\n",
    "    score = float(score)\n",
    "    \n",
    "    # Parse an alignment string into a list of tuples.\n",
    "    alignments = map(lambda s: tuple(map(int,s.split('-'))), alignments.strip().split(' '))\n",
    "    \n",
    "    return (k, source, target, segments, feature_map, score, alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two sets of lights so close to one another : intentional or just a silly error ?\n",
      "\n",
      "Zwei Anlagen so nah beieinander : Absicht oder Schildbürgerstreich ?\n",
      "\n",
      "[('zwei', (0, 12)), ('Lichter', (13, 24)), ('so', (25, 38)), ('eng miteinander verbunden', (39, 111)), (':', (112, 122)), ('vors\\xc3\\xa4tzliche', (123, 132)), ('oder nur eine', (133, 165)), ('dumme', (166, 169)), ('Fehler', (170, 170)), ('?', (171, 171))]\n",
      "{'UnknownWordPenalty0': [-100.0], 'TargetLM': [-36.6327], 'PhrasePenalty0': [5.0], 'TranslationModel0': [-4.18272, -3.41154, -6.11404, -6.027], 'PermutationExpectedKendallTau0': [0.865927, 0.0666667, 0.0666667], 'PermutationDistortion0': [0.0, 0.0, 0.0, 0.0], 'WordPenalty0': [-6.0], 'SourceLM': [-56.2177], 'InputFeature0': [0.0]}\n",
      "-21.8012\n"
     ]
    }
   ],
   "source": [
    "# Example: print all relevant information for sentence with id #2.\n",
    "print(inputs[2])\n",
    "print(references[2])\n",
    "(k, source, target, segments, feature_map, score, alignments) = parse_candidate(candidates[2000])\n",
    "print(segments)\n",
    "print(feature_vector)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
